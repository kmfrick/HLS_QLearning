
@book{sutton_reinforcement_2018,
	address = {Cambridge, MA London},
	edition = {Second edition},
	series = {Adaptive computation and machine learning},
	title = {Reinforcement learning: an introduction},
	isbn = {978-0-262-03924-6},
	shorttitle = {Reinforcement learning},
	abstract = {"Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms."--},
	language = {en},
	publisher = {The MIT Press},
	author = {Sutton, Richard S. and Barto, Andrew},
	year = {2018},
	note = {OCLC: 1050693967},
	file = {Sutton and Barto - 2018 - Reinforcement learning an introduction.pdf:/Users/kmfrick/Documents/Zotero/storage/L46WX3BS/Sutton and Barto - 2018 - Reinforcement learning an introduction.pdf:application/pdf},
}

@article{calvano_artificial_2020,
	title = {Artificial {Intelligence}, {Algorithmic} {Pricing}, and {Collusion}},
	volume = {110},
	issn = {0002-8282},
	url = {https://www.aeaweb.org/articles?id=10.1257/aer.20190623},
	doi = {10.1257/aer.20190623},
	abstract = {Increasingly, algorithms are supplanting human decision-makers in pricing goods and services. To analyze the possible consequences, we study experimentally the behavior of algorithms powered by Artificial Intelligence (Q-learning) in a workhorse oligopoly model of repeated price competition. We find that the algorithms consistently learn to charge supracompetitive prices, without communicating with one another. The high prices are sustained by collusive strategies with a finite phase of punishment followed by a gradual return to cooperation. This finding is robust to asymmetries in cost or demand, changes in the number of players, and various forms of uncertainty.},
	language = {en},
	number = {10},
	urldate = {2021-02-07},
	journal = {American Economic Review},
	author = {Calvano, Emilio and Calzolari, Giacomo and Denicolò, Vincenzo and Pastorello, Sergio},
	month = oct,
	year = {2020},
	keywords = {Belief, Communication, Firm Behavior: Theory, Market Structure, Pricing, and Design: Oligopoly and Other Forms of Market Imperfection, Search, Information and Knowledge, Learning, Monopolization Strategies, Oligopoly and Other Imperfect Markets, Unawareness, Monopoly},
	pages = {3267--3297},
	file = {Snapshot:/Users/kmfrick/Documents/Zotero/storage/2FG99FEN/articles.html:text/html},
}

@article{silva_parallel_2019,
	title = {Parallel {Implementation} of {Reinforcement} {Learning} {Q}-{Learning} {Technique} for {FPGA}},
	volume = {7},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2018.2885950},
	abstract = {Q-learning is an off-policy reinforcement learning technique, which has the main advantage of obtaining an optimal policy interacting with an unknown model environment. This paper proposes a parallel fixed-point Q-learning algorithm architecture implemented on field programmable gate arrays (FPGA) focusing on optimizing the system processing time. The convergence results are presented, and the processing time and occupied area were analyzed for different states and actions sizes scenarios and various fixed-point formats. The studies concerning the accuracy of the Q-learning technique response and resolution error associated with a decrease in the number of bits were also carried out for hardware implementation. The architecture implementation details were featured. The entire project was developed using the system generator platform (Xilinx), with a Virtex-6 xc6vcx240t-1ff1156 as the target FPGA.},
	journal = {IEEE Access},
	author = {Silva, L. M. D. Da and Torquato, M. F. and Fernandes, M. A. C.},
	year = {2019},
	note = {Conference Name: IEEE Access},
	keywords = {Clocks, Computer architecture, Field programmable gate arrays, FPGA, Hardware, Power demand, Q-learning, reconfigurable computing, reinforcement learning, Signal processing algorithms},
	pages = {2782--2798},
	file = {IEEE Xplore Full Text PDF:/Users/kmfrick/Documents/Zotero/storage/MJSU77NR/Silva et al. - 2019 - Parallel Implementation of Reinforcement Learning .pdf:application/pdf},
}

@article{watanabe_fpga-based_2020,
	title = {An {FPGA}-{Based} {On}-{Device} {Reinforcement} {Learning} {Approach} using {Online} {Sequential} {Learning}},
	url = {http://arxiv.org/abs/2005.04646},
	abstract = {DQN (Deep Q-Network) is a method to perform Q-learning for reinforcement learning using deep neural networks. DQNs require a large buffer and batch processing for an experience replay and rely on a backpropagation based iterative optimization, making them difficult to be implemented on resource-limited edge devices. In this paper, we propose a lightweight on-device reinforcement learning approach for low-cost FPGA devices. It exploits a recently proposed neural-network based on-device learning approach that does not rely on the backpropagation method but uses OS-ELM (Online Sequential Extreme Learning Machine) based training algorithm. In addition, we propose a combination of L2 regularization and spectral normalization for the on-device reinforcement learning so that output values of the neural network can be fit into a certain range and the reinforcement learning becomes stable. The proposed reinforcement learning approach is designed for Xilinx PYNQ-Z1 board as a low-cost FPGA platform. The evaluation results using OpenAI Gym demonstrate that the proposed algorithm and its FPGA implementation without data transfer overhead complete a CartPole-v0 task 29.76x and 125.88x faster than a conventional DQN-based approach when the number of hidden-layer nodes is 64.},
	urldate = {2021-03-15},
	journal = {arXiv:2005.04646 [cs, stat]},
	author = {Watanabe, Hirohisa and Tsukada, Mineto and Matsutani, Hiroki},
	month = aug,
	year = {2020},
	note = {arXiv: 2005.04646},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: 14 pages, 6 figures improved explanations},
	file = {arXiv Fulltext PDF:/Users/kmfrick/Documents/Zotero/storage/Y863DQGQ/Watanabe et al. - 2020 - An FPGA-Based On-Device Reinforcement Learning App.pdf:application/pdf;arXiv.org Snapshot:/Users/kmfrick/Documents/Zotero/storage/VSHZE2QP/2005.html:text/html},
}

@article{kretchmar_parallel_2002,
	title = {Parallel {Reinforcement} {Learning}},
	abstract = {We examine the dynamics of multiple reinforcement learning agents who are interacting with and learning from the same environment in parallel. Due to the stochasticity of the environment, each agent will have a different learning experience though they should all ultimately converge upon the same value function. The agents can accelerate the learning process by sharing information at periodic points during the learning process.},
	language = {en},
	journal = {The 6th World Conference on Systemics, Cybernetics, and Informatics},
	author = {Kretchmar, R Matthew},
	year = {2002},
	pages = {5},
	file = {Kretchmar - Parallel Reinforcement Learning.pdf:/Users/kmfrick/Documents/Zotero/storage/IDYNGYZT/Kretchmar - Parallel Reinforcement Learning.pdf:application/pdf},
}

@inproceedings{meng_qtaccel_2020,
	address = {New Orleans, LA, USA},
	title = {{QTAccel}: {A} {Generic} {FPGA} based {Design} for {Q}-{Table} based {Reinforcement} {Learning} {Accelerators}},
	isbn = {978-1-72817-445-7},
	shorttitle = {{QTAccel}},
	url = {https://ieeexplore.ieee.org/document/9150141/},
	doi = {10.1109/IPDPSW50202.2020.00024},
	urldate = {2021-03-15},
	booktitle = {2020 {IEEE} {International} {Parallel} and {Distributed} {Processing} {Symposium} {Workshops} ({IPDPSW})},
	publisher = {IEEE},
	author = {Meng, Yuan and Kuppannagari, Sanmukh and Rajat, Rachit and Srivastava, Ajitesh and Kannan, Rajgopal and Prasanna, Viktor},
	month = may,
	year = {2020},
	pages = {107--114},
}

@inproceedings{camelo_scalable_2016,
	title = {A {Scalable} {Parallel} {Q}-{Learning} {Algorithm} for {Resource} {Constrained} {Decentralized} {Computing} {Environments}},
	doi = {10.1109/MLHPC.2016.007},
	abstract = {The Internet of Things (IoT) is more and more becoming a platform for mission critical applications with stringent requirements in terms of response time and mobility. Therefore, a centralized High Performance Computing (HPC) environment is often not suitable or simply non-existing. Instead, there is a need for a scalable HPC model that supports the deployment of applications on the decentralized but resource constrained devices of the IoT. Recently, Reinforcement Learning (RL) algorithms have been used for decision making within applications by directly interacting with the environment. However, most RL algorithms are designed for centralized environments and are time and resource consuming. Therefore, they are not applicable to such constrained decentralized computing environments. In this paper, we propose a scalable Parallel Q-Learning (PQL) algorithm for resource constrained environments. By combining a table partition strategy together with a co-allocation of both processing and storage, we can significantly reduce the individual resource cost and, at the same time, guarantee convergence and minimize the communication cost. Experimental results show that our algorithm reduces the required training in proportion of the number of Q-Learning agents and, in terms of execution time, it is up to 24 times faster than several well-known PQL algorithms.},
	booktitle = {2016 2nd {Workshop} on {Machine} {Learning} in {HPC} {Environments} ({MLHPC})},
	author = {Camelo, M. and Famaey, J. and Latré, S.},
	month = nov,
	year = {2016},
	keywords = {Algorithm design and analysis, communication cost minimization, convergence, Convergence, decentralized resource constrained devices, execution time, Heuristic algorithms, Internet of Things, IoT, learning (artificial intelligence), Machine learning algorithms, multi-agent systems, parallel algorithms, Partitioning algorithms, processing co-allocation, Q-learning agents, resource constrained decentralized computing environment, resource cost reduction, Scalability, scalable HPC model, scalable parallel Q-learning algorithm, scalable PQL algorithm, storage co-allocation, table partition strategy, Training},
	pages = {27--35},
	file = {IEEE Xplore Abstract Record:/Users/kmfrick/Documents/Zotero/storage/4FCU66PE/7835792.html:text/html;IEEE Xplore Full Text PDF:/Users/kmfrick/Documents/Zotero/storage/P2986FJZ/Camelo et al. - 2016 - A Scalable Parallel Q-Learning Algorithm for Resou.pdf:application/pdf},
}

@phdthesis{watkins_learning_1989,
	title = {Learning from {Delayed} {Rewards}},
	url = {https://www.academia.edu/download/50360235/Learning_from_delayed_rewards_20161116-28282-v2pwvq.pdf},
	author = {Watkins, Chris},
	year = {1989},
}

@misc{fedorko_wfedorkomersenne-twister-hls_2018,
	title = {wfedorko/{Mersenne}-{Twister}-{HLS}},
	url = {https://github.com/wfedorko/Mersenne-Twister-HLS},
	abstract = {High Level Synthesis (Xilinx HLS) implementation of the popular Mersenne Twister pseudo-random number generator. This will generate a VHDL/Verilog module that streams 32 bit psuedo-randoms at 500 M...},
	urldate = {2021-05-10},
	author = {Fedorko, Wojtek},
	month = jun,
	year = {2018},
	note = {original-date: 2018-06-02T20:09:23Z},
}

@article{hu_voronoi-based_2020,
	title = {Voronoi-{Based} {Multi}-{Robot} {Autonomous} {Exploration} in {Unknown} {Environments} via {Deep} {Reinforcement} {Learning}},
	volume = {69},
	issn = {1939-9359},
	doi = {10.1109/TVT.2020.3034800},
	abstract = {Autonomous exploration is an important application of multi-vehicle systems, where a team of networked robots are coordinated to explore an unknown environment collaboratively. This technique has earned significant research interest due to its usefulness in search and rescue, fault detection and monitoring, localization and mapping, etc. In this paper, a novel cooperative exploration strategy is proposed for multiple mobile robots, which reduces the overall task completion time and energy costs compared to conventional methods. To efficiently navigate the networked robots during the collaborative tasks, a hierarchical control architecture is designed which contains a high-level decision making layer and a low-level target tracking layer. The proposed cooperative exploration approach is developed using dynamic Voronoi partitions, which minimizes duplicated exploration areas by assigning different target locations to individual robots. To deal with sudden obstacles in the unknown environment, an integrated deep reinforcement learning based collision avoidance algorithm is then proposed, which enables the control policy to learn from human demonstration data and thus improve the learning speed and performance. Finally, simulation and experimental results are provided to demonstrate the effectiveness of the proposed scheme.},
	number = {12},
	journal = {IEEE Transactions on Vehicular Technology},
	author = {Hu, Junyan and Niu, Hanlin and Carrasco, Joaquin and Lennox, Barry and Arvin, Farshad},
	month = dec,
	year = {2020},
	note = {Conference Name: IEEE Transactions on Vehicular Technology},
	keywords = {Autonomous exploration, collision avoidance, Collision avoidance, deep reinforcement learning, Mobile robots, multi-vehicle systems, Navigation, path planning, Reinforcement learning, Robot kinematics, Robot sensing systems},
	pages = {14413--14423},
	file = {IEEE Xplore Abstract Record:/Users/kmfrick/Documents/Zotero/storage/BSYVNZBD/9244647.html:text/html;IEEE Xplore Full Text PDF:/Users/kmfrick/Documents/Zotero/storage/HGPVKBDA/Hu et al. - 2020 - Voronoi-Based Multi-Robot Autonomous Exploration i.pdf:application/pdf},
}
